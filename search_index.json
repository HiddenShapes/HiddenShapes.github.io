[["index.html", "The shape of a theorem 1 Introductions", " The shape of a theorem (or Pappus revistited) Leonard Hardiman 1 Introductions This essay is all about a very special proof, one that I first learnt around six years ago. Every few months I suddenly remember it, and my mind gets blown all over again. No other proof has had that effect on me and I’ve never seen any attempts to popularise it online. Our journey will take us through a variety of mathematical disciplines such as projective geometry, topology and computer assisted proofs. Like all the best maths, it involves interactions between disparate fields. So what theorem is this proof going to prove? Pappus’. The fact that a result like Pappus’ theorem has such a great proof is a big surprise (to me at least). You see, I’ve never been one to say things like all of maths is beautiful. Don’t get me wrong, I love maths. Most of my friends would agree, I get too excited about some maths. But Pappus’ theorem… well, let’s take a look at it. Pappus’ theorem. Let \\(\\{A,B,C\\}\\) and \\(\\{X,Y,Z\\}\\) be two distinct triples of collinear points in \\(\\mathbb{R}^2\\) such that the lines \\((A,B)\\) and \\((X,Y)\\) don’t coincide. Then the points \\((A,Y) \\cap (B,X)\\), \\((A,Z) \\cap (C,X)\\) and \\((B,Z) \\cap (C,Y)\\) are also collinear. Note: Any brightly coloured dots in this figure (and subsequent ones) are interactive. Try moving them along the lines in the Pappus configuration above; you can also pan and zoom to get a better view if stuff moves off-screen. Before proceeding, let’s quickly clarify some terminology and notation. Three points are called collinear if they all lie on a common line, \\(\\mathbb{R}^2\\) is the set of pairs of real numbers and the \\(\\cap\\) symbol denotes intersection. From now on when technical terms come up, I’ll give their definition in a footnote like this one.1 Hopefully you can see what I was driving at before: at first glance, Pappus’ theorem is just a bit dry. It falls right in line with Pythagoras and Thales’ theorems as the kind of thing mathematicians are always trying to convince people don’t form the entirety of the subject. Well, our goal today is to uncover a truly fascinating bit of structure underlying this seemingly innocuous result, which will then lead to the promised earth-shaking proof. However, before that, we’re going to have a look at a different proof of Pappus’s theorem. And I warn you, this proof is even more boring that the statement. But that’s kind of the point. You’ll see. The smallest non-zero natural number.↩︎ "],["projective.html", "2 Points at infinity", " 2 Points at infinity If you are already familiar with the basics of projective geometry, feel free to skip this section. OK, time to come clean: proving Pappus’ theorem as stated previously is going to be tricky. Not because it’s a particularly hard theorem to prove, but because in its current form it isn’t strictly speaking true. Well, maybe that’s a bit harsh. The problem is that the theorem’s hypothesis assumes the existence of three points: \\((A,Y) \\cap (B,X)\\), \\((A,Z) \\cap (C,X)\\) and \\((B,Z) \\cap (C,Y)\\). Of course all these lines exist, but who’s to say that they intersect? Indeed, there are instances of the Pappus configuration where two of those lines are parallel. If we assume that they all intersect then the theorem works fine, but perhaps a more elegant solution exists. This is where things get projective. When presenting projective geometry, people tend to focus on certain applications. They bring up perspective, projections and even the Italian Renaissance. Sometimes such introductions can feel a little overwrought. As with many parts of maths, my preferred approach revolves around one simple idea: convenience. It’s really annoying that line don’t always intersect! We should change that. How? We just need to add a few new points. The initial idea is to notice that, if we imagine the plane \\(\\mathbb{R}^{2}\\) embedded into 3-dimensional space \\(\\mathbb{R}^{3}\\) at some fixed non-zero height (for example the \\(z=1\\) plane), then to every point in that plane we can associate the line which passes though both that point and the origin. While it is possible to assign an origin-passing line in \\(\\mathbb{R}^{3}\\) to every point in our embedded \\(\\mathbb{R}^{2}\\), there are a few extra origin-passing lines that will never be assigned to a point, namely the lines contained in the \\(z=0\\) plane. But that’s exactly what we’re after: some extra points! So let’s just rush optimistically ahead and make an extended plane called the projective plane, denoted \\(\\mathbb{RP}^{2}\\), whose points are exactly the lines in \\(\\mathbb{R}^{3}\\) that pass through the origin. Definition. (points in the projective plane) A point in \\(\\mathbb{RP}^{2}\\) is a line in \\(\\mathbb{R}^{3}\\) which passes through the origin. To visualise points in \\(\\mathbb{RP}^{2}\\) we will often draw their intersection with the plane \\(z=1\\), as illustrated by the figure above. So if to a point in \\(\\mathbb{RP}^{2}\\) coressponds to a line passing through the origin to what does a line in \\(\\mathbb{RP}^{2}\\) correspond to? Pleasingly, the answer is a plane passing through the origin. Definition. (lines in the projective plane) A line in \\(\\mathbb{RP}^{2}\\) is a plane in \\(\\mathbb{R}^{3}\\) which passes through the origin. In accordance with this terminology, three points in \\(\\mathbb{RP}^{2}\\) are said to be collinear if they all lie within the same plane. To visualise lines in \\(\\mathbb{RP}^{2}\\) we will often draw their intersection with the plane \\(z=1\\), as illustrated by the figure above. The first thing to note is that this new space perfectly solves our original problem. Two origin-passing planes always intersect in an origin passing line so we automatically know that two lines in \\(\\mathbb{RP}^{2}\\) intersect in a point in \\(\\mathbb{RP}^{2}\\). So how does this work when the two lines are parallel lines in \\(\\mathbb{R}^{2}\\)? Let’s have a look: This is exactly what we want, the projective lines intersect in one of our extra projective points i.e. an origin-passing line in \\(\\mathbb{R}^{3}\\) that doesn’t intersect our old embedded \\(\\mathbb{R}^{2}\\). Such points are called points at infinity, a name that seems fitting for the intersection points of parallel lines. The set of all points at infinity gives a projective line, the unique line at infinity: So we now know what a point in \\(\\mathbb{RP}^{2}\\) is, but practically speaking, how are we going to write one down? Well there’s a surprisingly straightforward system for this called homogeneous coordinates. The idea is to simply specify the coordinates of a single non-zero point in \\(\\mathbb{R}^{3}\\) that lies on the line. In the figure above you can move both the light blue dot (which changes the point in \\(\\mathbb{RP}^{2}\\)) and the dark blue dot (which gives possible homogeneous coordinates for that point). This figure illustrates how homogeneous coordinates are only defined up to scale.2 Despite this, they are extremely useful as the following proposition demonstrates. Proposition A. (collinearity in terms of homogeneous coordinates) Let \\(A\\), \\(B\\) and \\(C\\) be points in \\(\\mathbb{RP}^{2}\\) with corresponding homogeneous coordinates \\((a,b,c)\\), \\((d,e,f)\\) and \\((g,h,i)\\). Then the following are equivalent, \\(A\\), \\(B\\) and \\(C\\) are collinear \\(\\det M = 0\\) where \\(M\\) is the matrix \\({\\small \\begin{pmatrix} a&amp;b&amp;c \\\\[-2ex] d&amp;e&amp;f \\\\[-2ex] g&amp;h&amp;i \\end{pmatrix}}\\). Proof. By definition, \\(A\\), \\(B\\) and \\(C\\) are collinear if and only if the vectors \\((a,b,c)\\), \\((d,e,f)\\) and \\((g,h,i)\\) lie within a common plane. This is equivalent to requiring that there exists a linear relation3 between these three vectors, which is equivalent to requiring that \\(\\det M = 0\\). \\(\\square\\) Now that we have the basics of projective geometry in hand, there remains one technical result we should go over before proceeding with Pappus’ theorem. As the focus of this essay is uncovering cool facts hidden behind Pappus’ theorem, not establishing the foundations of projective geometry, we won’t see a proof of this result (although a reference will be provided). A good way of thinking about projective geometry is that it’s just another attempt to solve geometric problems with algebra (like Cartesian coordinates). What distinguishes projective geometry that we insist that the algebra be linear (that’s why everything is represented by lines and planes that pass through the origin). A big part of the subject is therefore simply bringing the tools of linear algebra into play. If you’ve every studied any linear algebra you’ll know that bases4 play an important role. The analogous notion in projective geometry is a collection of point in general position. Definition. (general position) Four distinct points in \\(\\mathbb{RP}^{2}\\) are said to be in general position when no three of them are collinear. So how do we actually use such a collection of points? Once again, we bring it back to linear algebra, having a collection of points in general position allows us to choose a special basis: Proposition B. (useful basis) Let \\(A,B,C,D \\in \\mathbb{RP}^{2}\\) be four distinct points in general position. Then there exists a basis \\(\\mathcal{B}\\) of \\(\\mathbb{R}^{3}\\) such that the homogeneous coordinates of \\(A,B,C\\) and \\(D\\) are \\((1,0,0)\\), \\((0,1,0)\\), \\((0,0,1)\\) and \\((1,1,1)\\) respectively. Proof. See Section 2.2.5 in Bobenko (2020), publicly available here. This proposition highlights how homogeneous coordinates aren’t a fundamental property of a point in \\(\\mathbb{RP}^{2}\\) and should only be though of with respect to a preferred basis (just like regular coordinates in linear algebra). With this final technicality in place, it’s time to move on. In other words, the homogeneous coordinates \\((a,b,c)\\) define the same point in \\(\\mathbb{RP}^{2}\\) as the homogeneous coordinates \\(\\lambda (a,b,c)\\) for any \\(\\lambda\\) in \\(\\mathbb{R}\\).↩︎ I.e. a way of adding together scalar multiples of two of them to give the third.↩︎ A basis is a set of vectors that is both linearly independent and spanning. For more, see here.↩︎ "],["proof.html", "3 Proving Pappus", " 3 Proving Pappus We’re now ready to prove Pappus’ theorem. But remember, this proof is the first, not very exciting proof; the dramatic proof won’t be until Section 5. Let’s restate the theorem once more, this time giving it the projective context it deserves. By the way, I wouldn’t recommend computationally checking every step of this proof for yourself: not only could that become fairly tedious, it also wouldn’t necessarily provide much insight. The reason this proof is being presented it to illustrate it’s structure: what kind of arguments make it up. We’ll discuss this structure once the proof is done. OK, let’s go. Pappus’ theorem. (homogeneous coordinate proof) Let \\(\\{A,B,C\\}\\) and \\(\\{X,Y,Z\\}\\) be two distinct triples of collinear points in \\(\\mathbb{RP}^{2}\\) such that the lines \\((A,B)\\) and \\((X,Y)\\) don’t coincide. Then the points \\(\\gamma = (A,Y) \\cap (B,X)\\), \\(\\beta = (A,Z) \\cap (C,X)\\) and \\(\\alpha = (B,Z) \\cap (C,Y)\\) are also collinear. Proof. As \\((A,C) \\neq (X,Z)\\) the points \\(A,X,\\alpha\\) and \\(\\gamma\\) are in general position. We can therefore apply Proposition B and get a basis \\(\\mathcal{B}\\) of \\(\\mathbb{R}^{3}\\) with respect to which \\(A,X,\\alpha\\) and \\(\\gamma\\) have homogeneous coordinates \\((1,0,0)\\), \\((0,1,0)\\), \\((0,0,1)\\) and \\((1,1,1)\\) respectively. Let \\((a,b,c), (d,e,f),(g,h,i),(j,k,l)\\) and \\((m,n,o)\\) be the homogeneous coordinates of \\(B,C,Y,Z\\) and \\(\\beta\\) respectively (using the basis \\(\\mathcal{B}\\)). Thanks to Proposition C we can translate the collinearities from the theorem’s hypothesis into the vanishing of certain discriminants: \\[ \\begin{array}[c]{ccccccccccc} \\{A,B,C\\} \\text{ collinear} &amp;\\iff&amp; \\det \\begin{pmatrix} 1&amp;0&amp;0\\\\ a&amp;b&amp;c \\\\ d&amp;e&amp;f \\end{pmatrix} =0; &amp;\\hspace{2em}&amp; \\{B,Z,\\alpha\\} \\text{ collinear} &amp;\\iff&amp; \\det \\begin{pmatrix} a&amp;b&amp;c\\\\ j&amp;k&amp;l \\\\ 0&amp;0&amp;1 \\end{pmatrix} =0; \\\\\\\\ \\{A,Y,\\gamma\\} \\text{ collinear} &amp;\\iff&amp; \\det \\begin{pmatrix} 1&amp;0&amp;0\\\\ g&amp;h&amp;i \\\\ 1&amp;1&amp;1 \\end{pmatrix} =0; &amp;\\hspace{2em}&amp; \\{C,X,\\beta\\} \\text{ collinear} &amp;\\iff&amp; \\det \\begin{pmatrix} d&amp;e&amp;f\\\\ 0&amp;1&amp;0 \\\\ m&amp;n&amp;o \\end{pmatrix} =0; \\\\\\\\ \\{A,Z,\\beta\\} \\text{ collinear} &amp;\\iff&amp; \\det \\begin{pmatrix} 1&amp;0&amp;0\\\\ j&amp;k&amp;l \\\\ m&amp;n&amp;o \\end{pmatrix} =0; &amp;\\hspace{2em}&amp; \\{C,Y,\\alpha\\} \\text{ collinear} &amp;\\iff&amp; \\det \\begin{pmatrix} d&amp;e&amp;f\\\\ g&amp;h&amp;i \\\\ 0&amp;0&amp;1 \\end{pmatrix} =0; \\\\\\\\ \\{B,X,\\gamma\\} \\text{ collinear} &amp;\\iff&amp; \\det \\begin{pmatrix} a&amp;b&amp;c\\\\ 0&amp;1&amp;0 \\\\ 1&amp;1&amp;1 \\end{pmatrix} =0; &amp;\\hspace{2em}&amp; \\{X,Y,Z\\} \\text{ collinear} &amp;\\iff&amp; \\det \\begin{pmatrix} 0&amp;1&amp;0\\\\ g&amp;h&amp;i \\\\ j&amp;k&amp;l \\end{pmatrix} =0. \\end{array} \\] As each matrix contains a standard vector row,5 each condition reduces to the vanishing of a 2×2 sub-matrix giving the following equalities, \\[ \\begin{array}[c]{llllllll} ce=bf, &amp; bj=ak, &amp; i=h, &amp; fm=do, &amp; ko=ln, &amp; dh=eg, &amp; a=c, &amp; gl=ij. \\end{array} \\] Multiplying all these equalities together and simplifying (possible by the non-degeneracy conditions6) reduces to \\(m=n\\) which is equivalent to \\[\\begin{align*} \\begin{pmatrix} 0&amp;0&amp;1\\\\ m&amp;n&amp;o \\\\ 1&amp;1&amp;1 \\end{pmatrix} =0 \\quad \\iff \\quad \\{\\alpha,\\beta,\\gamma\\} \\text{ collinear.} \\end{align*}\\] \\(\\square \\qquad\\) Perhaps the first thing to say about this proof is that it isn’t either particularly illuminating or memorable. It basically boils down to write down homogeneous coordinates for everything in sight, translate the hypothesis collinearities into algebraic relations between the homogeneous coordinates and then see if these imply the algebraic condition corresponding to the conclusion. In short, the proof is routine, technical and tedious, which makes it an ideal task for a computer. That’s why Dr. Jürgen Richter-Gebert wrote an algorithm that generated projective proof/theorem pairs based on this structure (Richter-Gebert 1993). In truth, the proof presented above does need some tweaking before it can be turned into an algorithm. Our choice of the basis \\(\\mathcal{B}\\) feels fairly specific to the Pappus configuration: there’s no reason why such a helpful basis should exist in another set-up. The solution to this problem is a bit beyond the scope of this essay, but in brief, one can use the Grassmann–Plücker relations to work in an equivalent manner without making a special choice of basis. One also has to systematise the argument that allows cancellations (i.e. how we proved that the coordinate \\(a\\) was non-zero). This is done by introducing the notion of non-degeneracy data (which in our case was the claim that the triples of points were distinct and \\((A,C) \\neq (X,Z)\\)), this data can then be shown to allow certain cancellations. To summarise, Richter-Gebert wrote an algorithm that took as input a collection of hypothesis intersections and collinearities (\\(\\mathcal{H}\\)) non-degeneracy data (\\(\\mathcal{B}\\)) a collection of conclusion intersections and collinearities (\\(\\mathcal{C}\\)) and would then decide whether or not \\(\\mathcal{C}\\) followed from \\(\\mathcal{H}\\) and \\(\\mathcal{B}\\). A proof produced via this algorithm is called a binomial proof. The first genuinely intriguing fact I learnt about Pappus’ theorem was that it’s the simplest non-trivial theorem which admits a binomial proof. This is all well and good, but the proofs produced by this algorithm are still going to admit the flaws we’ve already discussed. Perhaps a more illuminating (but equally powerful) proving strategy can be found. A row containing exactly one non-zero entry which is equal to \\(1\\).↩︎ The conditions that the triples of points are distinct and that \\((A,C) \\neq (X,Z)\\). To illustrate how this allows cancellation, we give an explicit argument that division by \\(a\\) is possible; analogous arguments hold for the other variables. By the non-degeneracy conditions, \\(C\\), \\(X\\) and \\(\\alpha\\) are not collinear. Therefore \\[ 0 \\neq \\det{\\small \\begin{pmatrix} a&amp;b&amp;c \\\\[-2ex] 0&amp;1&amp;0 \\\\[-2ex] 0&amp;0&amp;1 \\end{pmatrix}}=a. \\]↩︎ "],["ceva.html", "4 Variations on Ceva", " 4 Variations on Ceva The foundations of the rest of this essay are mainly composed of one result: Ceva’s theorem. Although it initially seems totally unrelated to Pappus’ theorem, our main result we eventually be a surprising and deep connection between the two. In many ways, Ceva’s theorem is a much simpler result than Pappus’. The only real subtlety is becoming familiar with signed lengths. Let \\(A\\) and \\(B\\) be two points on an oriented line. The signed length from \\(A\\) to \\(B\\) is given by \\[\\begin{align*} |AB| = \\begin{cases} \\hphantom{-}\\|AB\\| &amp; \\text{if the orientation points from $A$ to $B$} \\\\ -\\|AB\\| &amp; \\text{else}\\end{cases} \\end{align*}\\] where \\(\\|AB\\|\\) denotes the distance between \\(A\\) and \\(B\\). Theorem. (Ceva’s theorem) Let \\(ABC\\) be an oriented triangle in \\(\\mathbb{RP}^{2}\\). Let the lines \\(AD\\), \\(BD\\) and \\(CD\\) be drawn from the vertices to a common point \\(D\\) (called the intersection point) to meet opposite sides at \\(Y, Z\\) and \\(X\\) respectively. Then, the signed lengths of segments satisfy the relation, \\[\\begin{align*} \\frac{|AX|}{|XB|} \\cdot \\frac{|BY|}{|YC|} \\cdot \\frac{|CZ|}{|ZA|} = 1. \\end{align*}\\] Note: As the above can figure demonstrate (easier to see if you zoom out), \\(D\\) need not be within the interior of \\(ABC\\). Proof. OK, cards on the table, we’re not actually going to see a full proof of Ceva’s theorem. It’s not a hard theorem to prove, but the simplest arguments I know all use the notion of a projective transformation/invariant and that’s beyond the scope of this essay. A good reference for such proofs is Section 15.4 in Richter-Gebert (2011). However, proving the theorem’s Euclidean counterpart (i.e. we assume that \\(D\\) is not a point at infinity) is pretty easy and gives a good intuition for why the result holds.7 So we will go over that proof. If \\(D\\) is contained within the interior of \\(ABC\\) then all the terms in the left-hand-side of the desired equation are positive. On the other hand, if \\(D\\) is not contained within the interior of \\(ABC\\) then exactly two of the terms are negative. Therefore \\(\\frac{|AX|}{|XB|} \\cdot \\frac{|BY|}{|YC|} \\cdot \\frac{|CZ|}{|ZA|}\\) is always positive and we only have to show that \\[\\begin{align*} \\frac{\\|AX\\|}{\\|XB\\|} \\cdot \\frac{\\|BY\\|}{\\|YC\\|} \\cdot \\frac{\\|CZ\\|}{\\|ZA\\|} = 1. \\end{align*}\\] The area formula (area = height \\(\\times\\) base) tells us that, for any given height, a triangle’s area is proportional to the length of its base. We therefore have \\[\\begin{align*} \\frac{\\|BY\\|}{\\|YC\\|}=\\frac{\\Delta(B,D,Y)}{\\Delta(C,D,Y)}=\\frac{\\Delta(B,A,Y)}{\\Delta(C,A,Y)} \\end{align*}\\] where \\(\\Delta(B,D,Y)\\) denotes the area of \\(BDY\\). This implies \\[\\begin{align*} \\frac{\\|BY\\|}{\\|YD\\|}=\\frac{\\Delta(B,A,Y)-\\Delta(B,D,Y)}{\\Delta(C,A,Y)-\\Delta(C,D,Y)} = \\frac{\\Delta(B,A,D)}{\\Delta(C,A,D)}. \\end{align*}\\] Similarly we get \\[\\begin{align*} \\frac{\\|AX\\|}{\\|XB\\|}=\\frac{\\Delta(C,A,D)}{\\Delta(C,B,D)} \\quad \\text{and} \\quad \\frac{\\|CZ\\|}{\\|ZA\\|}=\\frac{\\Delta(C,B,D)}{\\Delta(B,A,D)}. \\end{align*}\\] Combining these three equalities gives \\[\\begin{align*} \\frac{\\|AX\\|}{\\|XB\\|} \\cdot \\frac{\\|BY\\|}{\\|YC\\|} \\cdot \\frac{\\|CZ\\|}{\\|ZA\\|} = \\frac{\\Delta(B,A,D)}{\\Delta(C,A,D)} \\cdot \\frac{\\Delta(C,A,D)}{\\Delta(C,B,D)} \\cdot \\frac{\\Delta(C,B,D)}{\\Delta(B,A,D)} = 1. \\end{align*}\\] \\(\\square \\qquad\\) One of the coolest things about Ceva’s therorem is that you can glue multiple copies of the theorem to itself. What does that mean? Well, let’s take a look at the following set up: Applying Ceva’s theorem on the left triangle tells us that \\(\\frac{|AX|}{|XB|} \\cdot \\frac{|BY|}{|YC|} \\cdot \\frac{|CZ|}{|ZA|} = 1\\); applying it on the right-hand triangle tells us that \\(\\frac{|A&#39;Z&#39;|}{|Z&#39;C|} \\cdot \\frac{|CY|}{|YB|} \\cdot \\frac{|BX&#39;|}{|X&#39;A&#39;|} =1\\). As \\(\\frac{|BY|}{|YC|} \\cdot \\frac{|CY|}{|YB|} = 1\\) (we note that requiring the quadrilateral \\(ABA&#39;C\\) be oriented forces the induced orientations on \\(ABC\\) and \\(A&#39;BC\\) to disagree along the shared edge \\(BC),\\) we get a result that only depends on points that lie on the perimeter of the quadrilateral \\(ABA&#39;C\\), namely \\[\\begin{align*} \\frac{|AX|}{|XB|} \\cdot \\frac{|BX&#39;|}{|X&#39;A&#39;|} \\cdot \\frac{|A&#39;Z&#39;|}{|Z&#39;C|}\\cdot \\frac{|CZ|}{|ZA|} = 1. \\end{align*}\\] But there’s no reason to stop with just two copies glued together. We can glue as many as we like! That said, the results won’t get much more interesting: we’ll just prove that an every-growing product of signed lengths along the perimeter evaluates to one. No, things get really interesting when we apply this perspective to triangulated surfaces.8 Let’s consider a triangulated sphere, i.e. a sphere whose surface has been divided up into triangles. Now let’s imagine that, one by one, we fill each of the triangles with compatible Ceva configurations until exactly one triangle remains unfilled. What would that look like? Here’s a sphere I prepared earlier: You can move the green dot labelled \\(D\\) to change the initial Ceva configuration and see how the others adapt (although moving the point into the exterior of its containing triangle with break things as drawing on a 3D surface is hard). Try and find the empty triangle (to get a good view of it I’d recommend using the full-screen button, shift-click to pan the camera and the mouse scroll to zoom in and out). If you’ve found the empty triangle, you’ll have seen that its corners are labelled \\(A,B,C\\) and the intersection points caused by the surrounding Ceva configurations are labelled \\(X,Y,Z\\). Exactly as we described before, applying Ceva’s theorem to each triangle, multiplying all the resulting equalities together and then cancelling all possible terms, leads to an equality that only depends on points that lie on the perimeter i.e. our one remaining triangle. And what is that equality? \\[\\begin{align*} \\frac{|AX|}{|XB|} \\cdot \\frac{|BY|}{|YC|} \\cdot \\frac{|CZ|}{|ZA|} = 1. \\end{align*}\\] Huh, that’s funny. It’s exactly the conclusion we’d draw if \\(X,Y\\) and \\(Z\\) were defined by a Ceva configuration on \\(ABC\\). If only Ceva’s Theorem had some kind of converse… Theorem. (Converse to Ceva’s theorem) Let \\(ABC\\) be a triangle, let \\(X \\in (A,B) , \\ Y \\in (B,C)\\) and \\(Z \\in (C,A)\\) such that \\[\\begin{align*} \\frac{|AX|}{|XB|} \\cdot \\frac{|BY|}{|YC|} \\cdot \\frac{|CZ|}{|ZA|} = 1 \\end{align*}\\] Then \\((A,Y) , \\ (B,Z)\\) and \\((C,X)\\) are concurrent. Proof. Let \\(D = (A,Y) \\cap (B,Z)\\) and \\(\\tilde{X}=(C,D) \\cap (A,B)\\). Then, by Ceva’s theorem, \\[\\begin{align*} \\frac{|A\\tilde{X}|}{|\\tilde{X}B|} \\cdot \\frac{|BY|}{|YC|} \\cdot \\frac{|CZ|}{|ZA|} = 1 \\text{, and therefore } \\frac{|A\\tilde{X}|}{|\\tilde{X}B|} = \\frac{|AX|}{|XB|} \\end{align*}\\] which, as \\(X\\) and \\(\\tilde{X}\\) both lie on \\((A,B)\\), implies \\(X = \\tilde{X}\\). \\(\\square\\) So, returning to our triangulated sphere, the equality we derived on the empty triangle exactly tells us that \\((A,Y) , \\ (B,Z)\\) and \\((C,X)\\) are concurrent. I find this really beautiful. Just by knowing Ceva’s theorem and how to glue copies of it together, we can automatically know the incidence result that \\((A,Y) , \\ (B,Z)\\) and \\((C,X)\\) are concurrent. An important point to realise is that no part of the argument we just made uses the fact that our surface was a sphere. We therefore get the following, more general (if also a bit more wordy), statement. Proposition. (Compatible Ceva configurations) Let \\(\\mathcal{S}\\) be a (compact, orientable) surface, let \\(\\mathcal{T}\\) be a triangulation of \\(S\\)9 and let \\(T\\) be a distinguished triangle in \\(\\mathcal{T}\\). We suppose that every triangle in \\(\\mathcal{T}\\) other that \\(T\\) is equipped with a Ceva configuration such that adjacent configurations are compatible along their shared edge. Then \\(T\\) admits a Ceva configuration that is compatible with all its adjacent configurations. Now we just have to prove Pappus’ theorem using this result. You can get a pretty good picture of what the theorem looks like when \\(D\\) is a point at infinity by moving \\(D\\) very far away from \\(ABC\\) in the interactive figure (zooming out, and then back in with the mouse scroll helps a lot for this).↩︎ Or, to be more precise, compact orientable triangulated 2-manifolds.↩︎ In other words, \\(\\mathcal{T}\\) is a collection of non-overlapping triangles that cover all of \\(S\\).↩︎ "],["shape.html", "5 The hidden shape", " 5 The hidden shape Let’s have one final look at the Pappus configuration, but this time let’s pretend that we don’t know the conclusion. The idea is to fix a triangle with exactly one vertex from each of the sets \\(\\{A,B,C\\}\\), \\(\\{X,Y,Z\\}\\) and \\(\\{\\alpha,\\beta,\\gamma\\}\\), e.g. \\(A\\alpha X\\). Now we look for Ceva configurations on \\(A\\alpha X\\). As the Ceva configuration is so simple finding them isn’t very hard, we just have to choose another point. Therefore, simply choosing the point \\(Z\\) gives one and we can represent this symbolically by drawing a triangle labelled by \\(Z\\). From now on, for any given point \\(P\\), the \\(P\\)-configuration is the Ceva configuration obtained by considering the triangle \\(A\\alpha X\\) together with the point \\(P\\). Let’s turn our focus to the \\(Y\\)-configuration. The crucial realisation is that, as \\(X,Y\\) and \\(Z\\) are collinear, the \\(Y\\)-configuration defines the same intersection point along \\((A\\alpha)\\) as the \\(Z\\)-configuration: This is exactly the compatibility condition that allows us to glue these two Ceva configurations along \\((A\\alpha)\\). To symbolise this, we draw two triangles correspondingly glued together. Similarly the \\(B\\)-configuration can also be glued into the picture. Adding the \\(C\\)-configuration is a bit trickier. Not only does it define the same intersection point along \\((AX)\\) as the \\(Y\\)-configuration (as \\(Y,\\alpha\\) and \\(C\\) are collinear), it also defines the same intersection point along \\((\\alpha X)\\) as the \\(B\\)-configuration (as \\(A,B\\) and \\(C\\) are collinear): We can therefore glue the \\(C\\)-configuration to the \\(Y\\)-configuration along \\((AX)\\) and also to the \\(B\\)-configuration along \\((\\alpha X)\\). To symbolise this we not only glue a triangle labelled \\(C\\) to the triangle labelled \\(Y\\), but also identify its \\((\\alpha X)\\) edge with the corresponding edge of the triangle labelled \\(B\\). As is standard notation, we represent the identification of different edges by colouring those edges with a common colour and indicating an orientation (which makes the identification unambiguous). In the same manner we add in the \\(\\gamma\\)-configuration. Here comes the final coup de grâce. Let’s consider the following triangulated surface. Our above discussion demonstrates how we can equip each triangle on \\(S\\) (except the top one) with a Ceva configuration coming from the points \\(C,Y,Z,B,\\gamma\\). Furthermore, our proposition on compatible Ceva configurations from the previous section tells us that there exists a point \\(\\tilde{\\beta}\\) such that the \\(\\tilde{\\beta}\\)-configuration is compatible with all its adjacent configurations. Let \\(P\\), \\(Q\\) and \\(R\\) be the intersection points that lie on \\((A\\alpha),(X\\alpha)\\) and \\((XA)\\) respectively. By definition, we have \\(\\tilde{\\beta} = (XP) \\cap (AQ)\\). However, we also know that the \\(\\beta\\)-configuration is compatible with the \\(C\\)-configuration (as \\(X\\),\\(\\beta\\) and \\(C\\) are collinear) and the \\(Z\\)-configuration (as \\(A\\),\\(\\beta\\) and \\(Z\\) are collinear). Therefore \\(\\beta = (XP) \\cap (AQ) = \\tilde{\\beta}\\).10 But that means the \\(\\beta\\)-configuration is also compatible with the \\(\\gamma\\)-configuration and that’s equivalent to \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) being collinear (as it implies that the lines \\((\\alpha, \\beta)\\) and \\((\\alpha, \\gamma)\\) share the points \\(\\alpha\\) and \\(R\\) and therefore coincide). We’ve done it, we’ve proved Pappus’ theorem! Remark. (explicitly drawing the Ceva configurations) You might be wondering why, during our proof, we only saw symbolic depictions of \\(A\\alpha Z\\) being glued to itself. Why didn’t we just explicitly draw all the compatible Ceva configurations. The reason is that, if you start with a standard looking Pappus configuration (like this one) then all the glued together Ceva configurations tend to look very chaotic. However, if you’re happy to start with a less standard looking Pappus configuration, you can explicitly draw all the resulting Ceva configurations into a coherent picture. This figure is actually coded backwards (i.e. all the Ceva configurations are declared first and then the Pappus configuration is worked out). For this reason all the left-hand-side points are immobile, but moving the point labelled \\(Z\\) on the right-hand-side varies the initial Ceva configurations. Now that the proof’s complete, let’s take a moment to bask in just how beautiful it is. It almost feels like, once you’ve established Ceva’s theorem and written down the triangulated surface \\(\\mathcal{S}\\), the theorem just proves itself. We didn’t really have to do anything, just interpret the consequences. Talking of \\(S\\), let’s try and get a better idea of what it actually looks like, starting by drawing it without the triangulation and labelling the glued edges. We consider two paths on \\(\\mathcal{S}\\): one, called \\(d\\), is obtained by following \\(a\\) and then \\(c\\) and the other, called \\(e\\), by following \\(b\\) and then the reverse of \\(a\\). Writing \\(\\mathcal{S}\\) with respect to these paths gives the following. If you still don’t recognise \\(\\mathcal{S}\\), check out this video. The thick blue lines in that video correspond to our paths, \\(d\\) and \\(e\\). That’s right, \\(\\mathcal{S}\\) is a torus. Personally, I find it cool that \\(\\mathcal{S}\\) is both a relatively simple, but topologically non-trivial surface. It’s also so deeply embedded into the theorem yet so hard to find. Have another look at the statement of Pappus’ theorem. It’s hard to believe that all along there was a secret torus behind the scenes! Identifying \\(\\mathcal{S}\\) also raises a natural question: what happens it we use a different surface? Surely things can’t get even better? This argument is simply explaining why if two Ceva configurations are both compatible with two other Ceva configurations, then they coincide.↩︎ "],["conclusion.html", "6 Concluding remarks", " 6 Concluding remarks The previous section suggests a fairly radical proof strategy: swap out the torus \\(\\mathcal{S}\\) for a different triangulated surface and see what you prove. Well, once again Prof. Jürgen Richter-Hebert rose to the occasion and developed these ideas into a computable algorithm (Richter-Gebert 2006). In truth, to get a truly impressive proof strategy you need to flesh out the material we’ve covered a bit. In particular, you need to be able to equip the triangles in your triangulation, not just with the Pappus configuration, but also with the Menelaus configuration (which has its own associated theorem). From now on we shall use the term TS proof to denote a proof that uses this strategy (the TS stands for Triangulated Surface). TS proofs have been around for a little while now, the earliest example I know of appears on page 68 in Coxeter and Greitzer (1967). To conclude this essay we explore aspects of the TS proof system in the style of the papers Richter-Gebert (2006) and Apel and Richter-Gebert (2010): by individually addressing a list of questions and topics. What happens if we glue our geometric configurations around a different triangulated surface? The answer to this question is exactly what one would hope for: you prove loads of different theorems. The wonderful fact is that the TS proof strategy isn’t just elegant, it’s also really powerful. Remember back in Section 3 where we discussed binomial proofs? Well, it turns out that there’s a deep connection between that proof strategy and the TS proof strategy. It’s been shown that any theorem which admits a binomial proof admits a TS proof (Apel and Richter-Gebert 2010). This includes many well-knows results: Miguel’s theorem, Desargues’ theorem, Thales’ theorem… For an impressive collection of such results, see Crapo and Richter-Gebert (1995) . The proof landscape. My favourite aspect of the TS proof strategy is the perspective it gives on the landscape of all projective incidence proofs. Sure binomial proofs already gave us a systematic way of exploring that landscape, but the TS proof strategy gives us one that’s so much richer. Exploring the landscape via binomial proofs essentially labels a proof by its hypotheses and conclusion, pretty standard stuff. Exploring with the TS proof strategy labels a proof by a triangulated surface (!) and assignment of either Ceva or Menelaus to each triangle. So many interesting questions fall out of this perspective. What happens if we glue two proofs together by gluing their triangulated surfaces together?11 Is there a connection between Pappus being the simplest non-trivial binomial theorem and the torus being the simplest topologically non-trivial surface? And finally, leading to our next question, can we really assign a shape to a theorem? Is there really such a thing as the shape of a theorem? This question is explicitly addressed by Apel and Richter-Gebert (2010). We reproduce their answer here: The manifold proofs [i.e. the TS proofs] come along with a natural topology. In particular we have seen two proofs of Pappos’s theorem and both beared the structure of a torus. Is the topological type of the proof an invariant of the theorem? So far we were not able to find Ceva-Menelaus proofs for Pappos’s theorem that had a different topological type (as long as we exclude the possibility of adding additional generic points). It sounds like we have evidence for the answer being yes, but the question remains open!12 This does mean that titling this essay The shape of a theorem was a bit tenuous… if any projective geometers are reading: please work on settling this question (preferably in the affirmative). Sure TS proofs have a certain elegance to them, but we already had binomial proofs and they seem simpler. If you agree with this sentiment, I’m afraid that might be partly due to my biases and how they come across in this account. I really wanted to include binomial proofs to illustrate the power of TS proofs, but I didn’t want them to become the focus. For this reason, I presented binomial proofs in as economical a fashion as I could, whereas I drew out every single detail of the TS proof of Pappus’ theorem.13 This could give readers the impression that binomial proofs are simpler than they really are. In particular, we didn’t even attempt to cover the Grassmann–Plücker relations and not proving Proposition B sweeps a lot (i.e. projective transformations) under the rug. Let’s put it this way: if you’d kidnapped me just after finishing my BSc in maths, isolated me on an island and told me that I couldn’t leave until I’d proved Pappus’ theorem, I think it would be more likely that I’d stumble into the TS-proof rather than reinvent the foundations of projective geometry and provide a binomial proof.14 A potential reason to prefer TS proofs is that they tend to provide more satisfying explanations. If one interrogates the binomial proof of Pappus’ theorem as to why the result holds, it can feel like the only answer provided is because the linear algebra says so. On the other hand, the TS proof responds with because you can equip this triangulated torus with compatible Ceva configurations. Whether or not the second answer is more illuminating is probably up for debate, but it is certainly more expressive. Where did all the projective geometry go? This is a fair question: we spent a lot of time setting up projective geometry at the beginning but then, as the essay progressed, it just sort of fell away. I did toy with the idea of removing any mention of projective geometry (which would’ve also allowed me to cut a certain amount of set-up). However, as previously mentioned, including binomial proofs felt important to sell the power of TS proofs. And you can’t even discuss binomial proofs without homogeneous coordinates. So I bit the bullet and wrote Section 2. On reflection, I think it turned out well. I like how simplifying the statement of Pappus’ theorem works as an initial motivation for introducing the projective framework. The fact that we didn’t need to refer back to projective ideas when developing our TS proof of Pappus’ theorem points, I feel, to a interesting aspect of TS proofs more generally. The proof strategy is essentially agnostic as to whether the configurations one attaches to the triangles are projective or Euclidean. In other words, whether the TS-proof strategy produces Euclidean or projective results comes down to whether we think of the corresponding Ceva and Menelaus configurations as Euclidean or projective. This seems different to the binomial proof strategy which, as far as I know, doesn’t have a Euclidean counterpart. An example of this is explored in Section 7 of Richter-Gebert (2006)↩︎ I’m sure we all agree this is the most exciting possibility.↩︎ Seriously, in most references the entirety of Section 5 is condensed into a few sentences.↩︎ Of course, it’s far more likely that I’d die of old age before finding either proof.↩︎ "],["acknowledgements.html", "Acknowledgements", " Acknowledgements First and foremost I offer thanks to Professor Jürgen Richter-Gebert. I learnt the story this essay endevous to tell from his excellent book Perspectives on Projective Geometry (Richter-Gebert 2011), in particular Chapters 1 and 15. I would also like to thank Professor Fran Burstall who both taught me projective geometry and introduced me to Richter-Gebert’s work. "],["references.html", "References", " References "]]
